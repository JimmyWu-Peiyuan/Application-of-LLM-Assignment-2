{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmywu/Documents/school/cmu/nl(x)/Application-of-LLM-Assignment-2/assignment2-rag/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all required Libraries\n",
    "import pandas as pd\n",
    "import transformers, torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ab245",
   "metadata": {},
   "source": [
    "# Read Passages from the Datasets and Drop rows if they are NA or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0e11a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uruguay (official full name in  ; pron.  , Eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is bordered by Brazil to the north, by Arge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montevideo was founded by the Spanish in the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The economy is largely based in agriculture (m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Transparency International, Urugu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              passage\n",
       "id                                                   \n",
       "0   Uruguay (official full name in  ; pron.  , Eas...\n",
       "1   It is bordered by Brazil to the north, by Arge...\n",
       "2   Montevideo was founded by the Spanish in the e...\n",
       "3   The economy is largely based in agriculture (m...\n",
       "4   According to Transparency International, Urugu..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\")\n",
    "\n",
    "print(passages.shape)\n",
    "passages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b32f3",
   "metadata": {},
   "source": [
    "# Tokenize Text and Generate Embeddings using Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39adefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 384)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode Text\n",
    "embeddings = embedding_model.encode(\n",
    "    passages['passage'].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "embeddings_np = np.asarray(embeddings)\n",
    "embeddings_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354e711",
   "metadata": {},
   "source": [
    "# Create Milvus Client and Insert your Embeddings to your DB\n",
    "- Make sure you define a schema for your collection (Points will be deducted if you fail to define a proper schema with ids, passage text, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ccfdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define every column of your schema\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "\n",
    "id_ = FieldSchema(name = \"id\", dtype = DataType.INT64, is_primary = True)        \n",
    "passage = FieldSchema(name = \"passage\", dtype = DataType.VARCHAR, max_length = 9000)\n",
    "embedding = FieldSchema(name = \"embedding\", dtype=DataType.FLOAT_VECTOR, dim = 384)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b4c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = CollectionSchema(\n",
    "    fields=[id_, passage, embedding],\n",
    "    description=\"rag_mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed3c24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmywu/Documents/school/cmu/nl(x)/Application-of-LLM-Assignment-2/assignment2-rag/venv/lib/python3.10/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = MilvusClient(\"rag_wikipedia_mini.db\")\n",
    "\n",
    "# Create the Collection with Collection Name = \"rag_mini\". Make sure you define the schema variable while creating the collection\n",
    "\n",
    "\n",
    "collection_name = \"rag_mini\"\n",
    "\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema\n",
    ")\n",
    "print(\"Collection created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b067a",
   "metadata": {},
   "source": [
    "**Convert your Pandas Dataframe to a list of dictionaries**\n",
    "- The Dictionary at least have 3 keys [id, passage, embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b60520f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, dict_keys(['id', 'passage', 'embedding']))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_data = [\n",
    "    {\"id\": int(i), \"passage\": passages.iloc[i][\"passage\"], \"embedding\": embeddings[i].tolist()}\n",
    "    for i in range(len(passages))\n",
    "]\n",
    "len(rag_data), rag_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba91d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'insert_count': 3200, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199]}\n"
     ]
    }
   ],
   "source": [
    "# Code to insert the data to your DB\n",
    "res = client.insert(collection_name=\"rag_mini\", data=rag_data)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba2a1a",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "- Do a Sanity Check on your database \n",
    "\n",
    "**Do not delete the below line during your submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e73b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity count: 41600\n",
      "Collection schema: {'collection_name': 'rag_mini', 'auto_id': False, 'num_shards': 0, 'description': 'rag_mini', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'passage', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 9000}}, {'field_id': 102, 'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'functions': [], 'aliases': [], 'collection_id': 0, 'consistency_level': 0, 'properties': {}, 'num_partitions': 0, 'enable_dynamic_field': False}\n"
     ]
    }
   ],
   "source": [
    "print(\"Entity count:\", client.get_collection_stats(\"rag_mini\")[\"row_count\"])\n",
    "print(\"Collection schema:\", client.describe_collection(\"rag_mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87920ab",
   "metadata": {},
   "source": [
    "# Steps to Fetch Results\n",
    "- Read the Question Dataset\n",
    "- Clean the Question Dataset if necessary (Drop Questions with NaN etc.)\n",
    "- Convert Each Query to a Vector Embedding (Use the same embedding model you used to embed your document)\n",
    "- Try for a Single Question First\n",
    "- Load Collection into Memory after creating Index for Search on your embedding field (This is an essential step before you can search in your db)\n",
    "- Search and Fetch Top N Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da659821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question     answer\n",
       "0  Was Abraham Lincoln the sixteenth President of...        yes\n",
       "1  Did Lincoln sign the National Banking Act of 1...        yes\n",
       "2                   Did his mother die of pneumonia?         no\n",
       "3      How many long was Lincoln's formal education?  18 months\n",
       "4       When did Lincoln begin his political career?       1832"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queries = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\")\n",
    "queries\n",
    "len(queries)\n",
    "\n",
    "queries = queries.reset_index()\n",
    "queries = queries[['question','answer']]\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "849c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the first question first\n",
    "query = queries.iloc[0][\"question\"]\n",
    "query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "import numpy as np\n",
    "query_embedding = np.asarray(query_embedding, dtype=\"float32\")\n",
    "print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c1383",
   "metadata": {},
   "source": [
    "#### Create Index on the embedding column on your DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22563d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created\n",
      "Collection loaded into memory\n"
     ]
    }
   ],
   "source": [
    "index_params = MilvusClient.prepare_index_params()\n",
    "\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"AUTOINDEX\",\n",
    "    metric_type=\"COSINE\",\n",
    "    params={\"M\": 8, \"efConstruction\": 64}\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    client.create_index(collection_name=\"rag_mini\", index_params=index_params)\n",
    "    print(\"Index created\")\n",
    "except Exception as e:\n",
    "    print(f\"Index creation result: {e}\")\n",
    "\n",
    "# Load collection into memory (required for search)\n",
    "client.load_collection(\"rag_mini\")\n",
    "print(\"Collection loaded into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "664364e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_ = client.search(\n",
    "    collection_name=\"rag_mini\",\n",
    "    data=query_embedding,               \n",
    "    anns_field=\"embedding\",\n",
    "    limit=50,\n",
    "    output_fields=[\"id\", \"passage\"],\n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 64}}\n",
    ")\n",
    "output = list(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa7bdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bf2f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfe7a24",
   "metadata": {},
   "source": [
    "## Now get the Context \n",
    "- Initially use the first passage ONLY as your context\n",
    "- In Later Experiments, you must try at least 2 different passage selection strategies (Top 3 / Top 5 / Top 10) and pass to your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eea31d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Young Abraham Lincoln'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "top = output_[0][0]\n",
    "\n",
    "context = top['entity']['passage']\n",
    "context[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b68ab",
   "metadata": {},
   "source": [
    "**Develop your Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4674fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are helpful assistant. Answer the question using the provided context. If the answer is not contained in the context, say you don't know. \n",
      " Context: Young Abraham Lincoln: \n",
      " Question: Was Abraham Lincoln the sixteenth President of the United States? \n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"You are helpful assistant. Answer the question using the provided context. If the answer is not contained in the context, say you don't know.\"\n",
    "\n",
    "prompt = f\"\"\"{system_prompt} \\n Context: {context}: \\n Question: {query} \"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bb4e9",
   "metadata": {},
   "source": [
    "# RAG Response for a Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context doesn't include that information.\n"
     ]
    }
   ],
   "source": [
    "# Load the LLM Model you want to use\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# key = str(key)\n",
    "llm_client = OpenAI(api_key = API_KEY)\n",
    "def generate_answer(prompt, model = \"gpt-5-nano-2025-08-07\"):\n",
    "    response = llm_client.responses.create(\n",
    "        model = model,\n",
    "        input = prompt\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "answer_text = generate_answer(prompt).output_text\n",
    "print(answer_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023b43c",
   "metadata": {},
   "source": [
    "# Generate Responses for all the Queries in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c67f9f",
   "metadata": {},
   "source": [
    "NOTE: Since processing each query takes at least 5-6 second, getting answers for all 918 rows of data is time-consume and costly. Therefore, for this analysis, we'll be only using 200 rows from the dataset to evaluate accuracy for the RAG protocol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc262ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt 0 length: 767\n",
      "processed 0 queries\n",
      "prompt 1 length: 1797\n",
      "prompt 2 length: 1131\n",
      "prompt 3 length: 1613\n",
      "prompt 4 length: 1163\n",
      "prompt 5 length: 917\n",
      "prompt 6 length: 835\n",
      "prompt 7 length: 1791\n",
      "prompt 8 length: 1671\n",
      "prompt 9 length: 763\n",
      "prompt 10 length: 1166\n",
      "processed 10 queries\n",
      "prompt 11 length: 1172\n",
      "prompt 12 length: 1082\n",
      "prompt 13 length: 744\n",
      "prompt 14 length: 727\n",
      "prompt 15 length: 1649\n",
      "prompt 16 length: 1676\n",
      "prompt 17 length: 1476\n",
      "prompt 18 length: 1124\n",
      "prompt 19 length: 1608\n",
      "prompt 20 length: 1936\n",
      "processed 20 queries\n",
      "prompt 21 length: 730\n",
      "prompt 22 length: 821\n",
      "prompt 23 length: 1197\n",
      "prompt 24 length: 964\n",
      "prompt 25 length: 743\n",
      "prompt 26 length: 1093\n",
      "prompt 27 length: 1153\n",
      "prompt 28 length: 843\n",
      "prompt 29 length: 1028\n",
      "prompt 30 length: 946\n",
      "processed 30 queries\n",
      "prompt 31 length: 1078\n",
      "prompt 32 length: 1125\n",
      "prompt 33 length: 872\n",
      "prompt 34 length: 1181\n",
      "prompt 35 length: 1306\n",
      "prompt 36 length: 1165\n",
      "prompt 37 length: 971\n",
      "prompt 38 length: 1392\n",
      "prompt 39 length: 834\n",
      "prompt 40 length: 1415\n",
      "processed 40 queries\n",
      "prompt 41 length: 736\n",
      "prompt 42 length: 856\n",
      "prompt 43 length: 1310\n",
      "prompt 44 length: 1394\n",
      "prompt 45 length: 920\n",
      "prompt 46 length: 833\n",
      "prompt 47 length: 781\n",
      "prompt 48 length: 1482\n",
      "prompt 49 length: 770\n",
      "prompt 50 length: 782\n",
      "processed 50 queries\n",
      "prompt 51 length: 1420\n",
      "prompt 52 length: 921\n",
      "prompt 53 length: 1474\n",
      "prompt 54 length: 926\n",
      "prompt 55 length: 916\n",
      "prompt 56 length: 798\n",
      "prompt 57 length: 1513\n",
      "prompt 58 length: 1347\n",
      "prompt 59 length: 1727\n",
      "prompt 60 length: 1247\n",
      "processed 60 queries\n",
      "prompt 61 length: 1117\n",
      "prompt 62 length: 1589\n",
      "prompt 63 length: 1495\n",
      "prompt 64 length: 837\n",
      "prompt 65 length: 826\n",
      "prompt 66 length: 1725\n",
      "prompt 67 length: 763\n",
      "prompt 68 length: 1358\n",
      "prompt 69 length: 880\n",
      "prompt 70 length: 1074\n",
      "processed 70 queries\n",
      "prompt 71 length: 977\n",
      "prompt 72 length: 1042\n",
      "prompt 73 length: 1159\n",
      "prompt 74 length: 1211\n",
      "prompt 75 length: 865\n",
      "prompt 76 length: 833\n",
      "prompt 77 length: 807\n",
      "prompt 78 length: 791\n",
      "prompt 79 length: 930\n",
      "prompt 80 length: 841\n",
      "processed 80 queries\n",
      "prompt 81 length: 1114\n",
      "prompt 82 length: 1127\n",
      "prompt 83 length: 1192\n",
      "prompt 84 length: 822\n",
      "prompt 85 length: 1233\n",
      "prompt 86 length: 784\n",
      "prompt 87 length: 1749\n",
      "prompt 88 length: 764\n",
      "prompt 89 length: 794\n",
      "prompt 90 length: 1844\n",
      "processed 90 queries\n",
      "prompt 91 length: 1811\n",
      "prompt 92 length: 768\n",
      "prompt 93 length: 754\n",
      "prompt 94 length: 767\n",
      "prompt 95 length: 791\n",
      "prompt 96 length: 749\n",
      "prompt 97 length: 770\n",
      "prompt 98 length: 741\n",
      "prompt 99 length: 781\n",
      "prompt 100 length: 1721\n",
      "processed 100 queries\n",
      "prompt 101 length: 757\n",
      "prompt 102 length: 753\n",
      "prompt 103 length: 959\n",
      "prompt 104 length: 776\n",
      "prompt 105 length: 1473\n",
      "prompt 106 length: 1839\n",
      "prompt 107 length: 779\n",
      "prompt 108 length: 1766\n",
      "prompt 109 length: 1830\n",
      "prompt 110 length: 757\n",
      "processed 110 queries\n",
      "prompt 111 length: 1443\n",
      "prompt 112 length: 769\n",
      "prompt 113 length: 1014\n",
      "prompt 114 length: 1037\n",
      "prompt 115 length: 1317\n",
      "prompt 116 length: 987\n",
      "prompt 117 length: 1337\n",
      "prompt 118 length: 785\n",
      "prompt 119 length: 1350\n",
      "prompt 120 length: 1033\n",
      "processed 120 queries\n",
      "prompt 121 length: 1181\n",
      "prompt 122 length: 1302\n",
      "prompt 123 length: 801\n",
      "prompt 124 length: 1323\n",
      "prompt 125 length: 1419\n",
      "prompt 126 length: 1749\n",
      "prompt 127 length: 946\n",
      "prompt 128 length: 987\n",
      "prompt 129 length: 1237\n",
      "prompt 130 length: 1050\n",
      "processed 130 queries\n",
      "prompt 131 length: 1014\n",
      "prompt 132 length: 1198\n",
      "prompt 133 length: 805\n",
      "prompt 134 length: 830\n",
      "prompt 135 length: 1112\n",
      "prompt 136 length: 765\n",
      "prompt 137 length: 1674\n",
      "prompt 138 length: 1030\n",
      "prompt 139 length: 1339\n",
      "prompt 140 length: 1594\n",
      "processed 140 queries\n",
      "prompt 141 length: 1191\n",
      "prompt 142 length: 1141\n",
      "prompt 143 length: 1259\n",
      "prompt 144 length: 1313\n",
      "prompt 145 length: 1002\n",
      "prompt 146 length: 1028\n",
      "prompt 147 length: 1058\n",
      "prompt 148 length: 857\n",
      "prompt 149 length: 886\n",
      "prompt 150 length: 865\n",
      "processed 150 queries\n",
      "prompt 151 length: 1367\n",
      "prompt 152 length: 990\n",
      "prompt 153 length: 1019\n",
      "prompt 154 length: 925\n",
      "prompt 155 length: 1194\n",
      "prompt 156 length: 898\n",
      "prompt 157 length: 796\n",
      "prompt 158 length: 989\n",
      "prompt 159 length: 841\n",
      "prompt 160 length: 890\n",
      "processed 160 queries\n",
      "prompt 161 length: 1030\n",
      "prompt 162 length: 1190\n",
      "prompt 163 length: 900\n",
      "prompt 164 length: 915\n",
      "prompt 165 length: 1046\n",
      "prompt 166 length: 1237\n",
      "prompt 167 length: 918\n",
      "prompt 168 length: 1192\n",
      "prompt 169 length: 993\n",
      "prompt 170 length: 946\n",
      "processed 170 queries\n",
      "prompt 171 length: 1412\n",
      "prompt 172 length: 973\n",
      "prompt 173 length: 1149\n",
      "prompt 174 length: 1199\n",
      "prompt 175 length: 1180\n",
      "prompt 176 length: 1194\n",
      "prompt 177 length: 885\n",
      "prompt 178 length: 735\n",
      "prompt 179 length: 1208\n",
      "prompt 180 length: 1746\n",
      "processed 180 queries\n",
      "prompt 181 length: 1433\n",
      "prompt 182 length: 1130\n",
      "prompt 183 length: 1189\n",
      "prompt 184 length: 1183\n",
      "prompt 185 length: 1182\n",
      "prompt 186 length: 898\n",
      "prompt 187 length: 1141\n",
      "prompt 188 length: 1195\n",
      "prompt 189 length: 1162\n",
      "prompt 190 length: 1424\n",
      "processed 190 queries\n",
      "prompt 191 length: 1524\n",
      "prompt 192 length: 1445\n",
      "prompt 193 length: 736\n",
      "prompt 194 length: 831\n",
      "prompt 195 length: 1252\n",
      "prompt 196 length: 893\n",
      "prompt 197 length: 1288\n",
      "prompt 198 length: 1262\n",
      "prompt 199 length: 979\n"
     ]
    }
   ],
   "source": [
    "TOP_K_CONTEXT = 5\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "llm_client = OpenAI(api_key = API_KEY)\n",
    "def generate_answer(prompt, model = \"gpt-5-nano-2025-08-07\"):\n",
    "    response = llm_client.responses.create(\n",
    "        model = model,\n",
    "        input = prompt\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def generate_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "        INSTRUCTIONS FOR ANSWERING QUESTIONS:\n",
    "        TASK: Answer the given question using ONLY the provided context.\n",
    "        PROCESS:\n",
    "        1. Read the context thoroughly\n",
    "        2. Determine if the context contains sufficient information to answer\n",
    "        3. If sufficient: Provide a direct, accurate answer (yes/no/one word answer)\n",
    "        4. If insufficient: State \"unknown\"\n",
    "        REQUIREMENTS:\n",
    "        - Base your answer strictly on the provided context\n",
    "        - Do not use external knowledge\n",
    "        - Be concise but complete\n",
    "        - Maintain factual accuracy\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Following the instructions above, provide your answer:\n",
    "    \"\"\"\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "answers = []\n",
    "\n",
    "queries_200 = queries.head(200)\n",
    "\n",
    "for i, row in queries_200.iterrows():\n",
    "    q = row['question']\n",
    "    q_emb = embedding_model.encode([q], normalize_embeddings = True)\n",
    "    q_emb = np.asarray(q_emb,dtype = \"float32\")\n",
    "    output_ = client.search(\n",
    "        collection_name=\"rag_mini\",\n",
    "        data=q_emb,               \n",
    "        anns_field=\"embedding\",\n",
    "        limit=5,\n",
    "        output_fields=[\"id\", \"passage\"],\n",
    "        search_params={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 16}}\n",
    "\n",
    "    )\n",
    "    context = output_[0][0]['entity']['passage']\n",
    "    \n",
    "    prompt = generate_prompt(context, q)\n",
    "    print(f\"prompt {i} length: {len(prompt)}\")\n",
    "    answer = generate_answer(prompt)\n",
    "    answers.append(answer.output_text)\n",
    "    if i%10 == 0:\n",
    "        print(f\"processed {i} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a549b2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1b77e",
   "metadata": {},
   "source": [
    "# Finding out the Basic QA Metrics (F1 score, EM score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dba9353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F1 and EM scores...\n",
      "==================================================\n",
      "\n",
      "Example 1:\n",
      "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Ground Truth: yes\n",
      "Generated Answer: unknown\n",
      "F1 Score: 0.0000\n",
      "EM Score: 0\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "OVERALL RESULTS:\n",
      "==================================================\n",
      "Average F1 Score: 0.4675\n",
      "Average EM Score: 0.4000\n",
      "Total Questions Evaluated: 200\n",
      "==================================================\n",
      "\n",
      "F1 Score Statistics:\n",
      "Min F1: 0.0000\n",
      "Max F1: 1.0000\n",
      "Std F1: 0.4732\n",
      "\n",
      "EM Score Statistics:\n",
      "Exact Matches: 80\n",
      "Exact Match Rate: 40.00%\n"
     ]
    }
   ],
   "source": [
    "# F1 Score and EM Score Implementation\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.IGNORECASE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Calculate F1 score between prediction and ground truth.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    \n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Calculate exact match score between prediction and ground truth.\"\"\"\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "# Calculate F1 and EM scores for all 200 queries\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "print(\"Calculating F1 and EM scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(len(queries_200)):\n",
    "    question = queries_200.iloc[i]['question']\n",
    "    generated_answer = answers[i]\n",
    "    ground_truth = queries_200.iloc[i]['answer'] \n",
    "    \n",
    "    f1 = f1_score(generated_answer, ground_truth)\n",
    "    em = exact_match_score(generated_answer, ground_truth)\n",
    "    \n",
    "    f1_scores.append(f1)\n",
    "    em_scores.append(em)\n",
    "    \n",
    "    # Print first few examples\n",
    "    if i == 0:\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Generated Answer: {generated_answer}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"EM Score: {em}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_f1 = np.mean(f1_scores)\n",
    "overall_em = np.mean(em_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"OVERALL RESULTS:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Average F1 Score: {overall_f1:.4f}\")\n",
    "print(f\"Average EM Score: {overall_em:.4f}\")\n",
    "print(f\"Total Questions Evaluated: {len(queries_200)}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"\\nF1 Score Statistics:\")\n",
    "print(f\"Min F1: {min(f1_scores):.4f}\")\n",
    "print(f\"Max F1: {max(f1_scores):.4f}\")\n",
    "print(f\"Std F1: {np.std(f1_scores):.4f}\")\n",
    "\n",
    "print(f\"\\nEM Score Statistics:\")\n",
    "print(f\"Exact Matches: {sum(em_scores)}\")\n",
    "print(f\"Exact Match Rate: {sum(em_scores)/len(em_scores)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53fba45",
   "metadata": {},
   "source": [
    "# Add Two Advaned RAG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df3346a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing reranker...\n",
      "Reranker ready!\n"
     ]
    }
   ],
   "source": [
    "# Advanced RAG Integration - Query Rewriting and Reranking\n",
    "# Following the same structure as your existing code\n",
    "\n",
    "# Install required package first:\n",
    "# pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import re\n",
    "\n",
    "# Initialize reranker (one-time setup)\n",
    "print(\"Initializing reranker...\")\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Reranker ready!\")\n",
    "\n",
    "def rewrite_query(query):\n",
    "    \"\"\"\n",
    "    Query Rewriting: Generate multiple query variations\n",
    "    \"\"\"\n",
    "    # Extract key terms from query\n",
    "    words = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    stop_words = {'what', 'who', 'when', 'where', 'why', 'how', 'is', 'are', 'was', 'were', 'the', 'a', 'an', 'and', 'or', 'but'}\n",
    "    key_terms = [w for w in words if len(w) > 2 and w not in stop_words]\n",
    "    \n",
    "    # Generate query variations\n",
    "    variations = [query]  # Always include original query\n",
    "    \n",
    "    if key_terms:\n",
    "        main_term = key_terms[0]\n",
    "        variations.extend([\n",
    "            f\"Information about {main_term}\",\n",
    "            f\"Details regarding {main_term}\",\n",
    "            f\"What is {main_term}?\",\n",
    "            f\"Tell me about {main_term}\"\n",
    "        ])\n",
    "    \n",
    "    return variations[:4]  # Limit to 4 variations\n",
    "\n",
    "def enhanced_search_with_reranking(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Enhanced search combining query rewriting and reranking\n",
    "    \"\"\"\n",
    "    # Step 1: Query rewriting\n",
    "    query_variations = rewrite_query(query)\n",
    "    print(f\"Generated {len(query_variations)} query variations\")\n",
    "    \n",
    "    # Step 2: Search for each variation (same as your existing search)\n",
    "    all_docs = []\n",
    "    for variation in query_variations:\n",
    "        q_emb = embedding_model.encode([variation], normalize_embeddings=True)\n",
    "        q_emb = np.asarray(q_emb, dtype=\"float32\")\n",
    "        \n",
    "        output_ = client.search(\n",
    "            collection_name=\"rag_mini\",\n",
    "            data=q_emb,\n",
    "            anns_field=\"embedding\",\n",
    "            limit=20,  # Get more docs for reranking\n",
    "            output_fields=[\"id\", \"passage\"],\n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 16}}\n",
    "        )\n",
    "        \n",
    "        for result in output_[0]:\n",
    "            all_docs.append({\n",
    "                'id': result['entity']['id'],\n",
    "                'passage': result['entity']['passage'],\n",
    "                'distance': result['distance']\n",
    "            })\n",
    "    \n",
    "    # Step 3: Remove duplicates (keep best distance for each doc)\n",
    "    unique_docs = {}\n",
    "    for doc in all_docs:\n",
    "        doc_id = doc['id']\n",
    "        if doc_id not in unique_docs or doc['distance'] < unique_docs[doc_id]['distance']:\n",
    "            unique_docs[doc_id] = doc\n",
    "    \n",
    "    unique_documents = list(unique_docs.values())\n",
    "    print(f\"Retrieved {len(unique_documents)} unique documents\")\n",
    "    \n",
    "    # Step 4: Reranking\n",
    "    if len(unique_documents) > 1:\n",
    "        query_doc_pairs = [(query, doc['passage']) for doc in unique_documents]\n",
    "        relevance_scores = reranker.predict(query_doc_pairs)\n",
    "        \n",
    "        for i, doc in enumerate(unique_documents):\n",
    "            doc['relevance_score'] = float(relevance_scores[i])\n",
    "        \n",
    "        # Sort by relevance score (higher is better)\n",
    "        reranked_docs = sorted(unique_documents, key=lambda x: x['relevance_score'], reverse=True)\n",
    "    else:\n",
    "        reranked_docs = unique_documents\n",
    "    \n",
    "    return reranked_docs[:top_k]\n",
    "\n",
    "# Enhanced answer generation function (same structure as your existing code)\n",
    "def generate_enhanced_answer(context_docs, question):\n",
    "    \"\"\"\n",
    "    Generate answer using enhanced context (multiple documents)\n",
    "    \"\"\"\n",
    "    # Combine top documents as context\n",
    "    context = \"\\n\\n\".join([doc['passage'] for doc in context_docs])\n",
    "    \n",
    "    # Use your existing prompt structure\n",
    "    prompt = generate_prompt(context, question)\n",
    "    \n",
    "    # Use your existing generate_answer function\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer.output_text\n",
    "\n",
    "# Test the enhanced system\n",
    "# def test_enhanced_rag():\n",
    "#     \"\"\"Test the enhanced RAG system with a sample query\"\"\"\n",
    "    \n",
    "#     sample_query = \"Who was Abraham Lincoln?\"\n",
    "    \n",
    "#     print(\"Testing Enhanced RAG System\")\n",
    "#     print(\"=\" * 60)\n",
    "    \n",
    "#     # Get enhanced results\n",
    "#     enhanced_docs = enhanced_search_with_reranking(sample_query, top_k=3)\n",
    "    \n",
    "#     print(f\"\\nTop {len(enhanced_docs)} documents after reranking:\")\n",
    "#     for i, doc in enumerate(enhanced_docs):\n",
    "#         print(f\"\\nDocument {i+1}:\")\n",
    "#         relevance_score = doc.get('relevance_score')\n",
    "#         if relevance_score is not None:\n",
    "#             print(f\"Relevance Score: {relevance_score:.3f}\")\n",
    "#         else:\n",
    "#             print(\"Relevance Score: N/A\")\n",
    "#         print(f\"Content: {doc['passage'][:200]}...\")\n",
    "    \n",
    "#     # Generate answer\n",
    "#     print(f\"\\nGenerating answer for: {sample_query}\")\n",
    "#     answer = generate_enhanced_answer(enhanced_docs, sample_query)\n",
    "#     print(f\"Answer: {answer}\")\n",
    "    \n",
    "#     return enhanced_docs, answer\n",
    "\n",
    "# # Run the test\n",
    "# enhanced_results, enhanced_answer = test_enhanced_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b8464c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing queries with Enhanced RAG (Query Rewriting + Reranking)...\n",
      "======================================================================\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Processed 0 queries\n",
      "Sample: Was Abraham Lincoln the sixteenth President of the... -> Yes...\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Processed 10 queries\n",
      "Sample: Did Lincoln start his political career in 1832?... -> Yes...\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Processed 20 queries\n",
      "Sample: Was Lincoln chosen as a presidential candidate in ... -> Yes...\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Processed 30 queries\n",
      "Sample: When did he become a professor?... -> 1833...\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Processed 40 queries\n",
      "Sample: Who determined the dependence of the boiling of wa... -> Celsius...\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Processed 50 queries\n",
      "Sample: Is the Celsius crater on the Moon named after him ... -> Yes...\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Processed 60 queries\n",
      "Sample: Which defense mechanism uses colour or shape to de... -> Mimicry...\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Processed 70 queries\n",
      "Sample: What is the study of beetles called?... -> coleopterology...\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 9 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Processed 80 queries\n",
      "Sample: Is there a thriving industry in the collection of ... -> Yes...\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 5 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Processed 90 queries\n",
      "Sample: When did Coolidge meet and marry Grace Anna Goodhu... -> 1905...\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 3 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "\n",
      "Completed processing 100 queries with Enhanced RAG!\n"
     ]
    }
   ],
   "source": [
    "TOP_K_CONTEXT = 3  \n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "print(\"Processing queries with Enhanced RAG (Query Rewriting + Reranking)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "enhanced_answers = []\n",
    "queries_100 = queries.head(100)\n",
    "for i, row in queries_100.iterrows():\n",
    "    q = row['question']\n",
    "    # Enhanced search with query rewriting and reranking\n",
    "    enhanced_docs = enhanced_search_with_reranking(q, top_k=TOP_K_CONTEXT)\n",
    "    \n",
    "    # Generate answer using enhanced context\n",
    "    answer = generate_enhanced_answer(enhanced_docs, q)\n",
    "    enhanced_answers.append(answer)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} queries\")\n",
    "        print(f\"Sample: {q[:50]}... -> {answer[:100]}...\")\n",
    "\n",
    "print(f\"\\nCompleted processing {len(enhanced_answers)} queries with Enhanced RAG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3770efb",
   "metadata": {},
   "source": [
    "## Evaluation of Advaned RAG Features (F1 score and EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c233314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ADVANCED RAG RESULTS:\n",
      "==================================================\n",
      "Average F1 Score: 0.6320\n",
      "Average EM Score: 0.5400\n",
      "Total Questions Evaluated: 100\n",
      "==================================================\n",
      "\n",
      "Advanced RAG F1 Score Statistics:\n",
      "Min F1: 0.0000\n",
      "Max F1: 1.0000\n",
      "Std F1: 0.4437\n",
      "\n",
      "Advanced RAG EM Score Statistics:\n",
      "Exact Matches: 54\n",
      "Exact Match Rate: 54.00%\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Basic RAG vs Advanced RAG\n",
      "============================================================\n",
      "Basic RAG (200 queries):\n",
      "  Average F1 Score: 0.4675\n",
      "  Average EM Score: 0.4000\n",
      "  Exact Match Rate: 40.00%\n",
      "\n",
      "Advanced RAG (100 queries):\n",
      "  Average F1 Score: 0.6320\n",
      "  Average EM Score: 0.5400\n",
      "  Exact Match Rate: 54.00%\n",
      "\n",
      "IMPROVEMENT ANALYSIS:\n",
      "F1 Score Improvement: +0.1645 (+35.2%)\n",
      "EM Score Improvement: +0.1400 (+35.0%)\n",
      "\n",
      " Advanced RAG shows 0.1645 improvement in F1 score\n",
      " Advanced RAG shows 0.1400 improvement in EM score\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate F1 and EM scores for Advanced RAG (100 queries)\n",
    "advanced_f1_scores = []\n",
    "advanced_em_scores = []\n",
    "\n",
    "for i in range(len(queries_100)):\n",
    "    question = queries_100.iloc[i]['question']\n",
    "    generated_answer = enhanced_answers[i]  # Assuming enhanced_answers contains 100 answers\n",
    "    ground_truth = queries_100.iloc[i]['answer']\n",
    "    \n",
    "    f1 = f1_score(generated_answer, ground_truth)\n",
    "    em = exact_match_score(generated_answer, ground_truth)\n",
    "    \n",
    "    advanced_f1_scores.append(f1)\n",
    "    advanced_em_scores.append(em)\n",
    "\n",
    "# Calculate overall metrics for Advanced RAG\n",
    "advanced_overall_f1 = np.mean(advanced_f1_scores)\n",
    "advanced_overall_em = np.mean(advanced_em_scores)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ADVANCED RAG RESULTS:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Average F1 Score: {advanced_overall_f1:.4f}\")\n",
    "print(f\"Average EM Score: {advanced_overall_em:.4f}\")\n",
    "print(f\"Total Questions Evaluated: {len(queries_100)}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Additional statistics for Advanced RAG\n",
    "print(f\"\\nAdvanced RAG F1 Score Statistics:\")\n",
    "print(f\"Min F1: {min(advanced_f1_scores):.4f}\")\n",
    "print(f\"Max F1: {max(advanced_f1_scores):.4f}\")\n",
    "print(f\"Std F1: {np.std(advanced_f1_scores):.4f}\")\n",
    "\n",
    "print(f\"\\nAdvanced RAG EM Score Statistics:\")\n",
    "print(f\"Exact Matches: {sum(advanced_em_scores)}\")\n",
    "print(f\"Exact Match Rate: {sum(advanced_em_scores)/len(advanced_em_scores)*100:.2f}%\")\n",
    "\n",
    "# Comparison with Basic RAG (assuming you have basic_answers for 200 queries)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPARISON: Basic RAG vs Advanced RAG\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Basic RAG metrics (from your existing evaluation)\n",
    "basic_overall_f1 = np.mean(f1_scores)  # Your existing f1_scores from 200 queries\n",
    "basic_overall_em = np.mean(em_scores)   # Your existing em_scores from 200 queries\n",
    "\n",
    "print(f\"Basic RAG (200 queries):\")\n",
    "print(f\"  Average F1 Score: {basic_overall_f1:.4f}\")\n",
    "print(f\"  Average EM Score: {basic_overall_em:.4f}\")\n",
    "print(f\"  Exact Match Rate: {sum(em_scores)/len(em_scores)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nAdvanced RAG (100 queries):\")\n",
    "print(f\"  Average F1 Score: {advanced_overall_f1:.4f}\")\n",
    "print(f\"  Average EM Score: {advanced_overall_em:.4f}\")\n",
    "print(f\"  Exact Match Rate: {sum(advanced_em_scores)/len(advanced_em_scores)*100:.2f}%\")\n",
    "\n",
    "# Improvement analysis\n",
    "f1_improvement = advanced_overall_f1 - basic_overall_f1\n",
    "em_improvement = advanced_overall_em - basic_overall_em\n",
    "\n",
    "print(f\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "print(f\"F1 Score Improvement: {f1_improvement:+.4f} ({f1_improvement/basic_overall_f1*100:+.1f}%)\")\n",
    "print(f\"EM Score Improvement: {em_improvement:+.4f} ({em_improvement/basic_overall_em*100:+.1f}%)\")\n",
    "\n",
    "# Performance summary\n",
    "if f1_improvement > 0:\n",
    "    print(f\"\\n Advanced RAG shows {f1_improvement:.4f} improvement in F1 score\")\n",
    "else:\n",
    "    print(f\"\\n Advanced RAG shows {f1_improvement:.4f} decrease in F1 score\")\n",
    "\n",
    "if em_improvement > 0:\n",
    "    print(f\" Advanced RAG shows {em_improvement:.4f} improvement in EM score\")\n",
    "else:\n",
    "    print(f\" Advanced RAG shows {em_improvement:.4f} decrease in EM score\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAI(api_key = API_KEY)\n",
    "def generate_answer(prompt, model = \"gpt-5-nano-2025-08-07\"):\n",
    "    response = llm_client.responses.create(\n",
    "        model = model,\n",
    "        input = prompt\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def generate_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "        INSTRUCTIONS FOR ANSWERING QUESTIONS:\n",
    "        TASK: Answer the given question using ONLY the provided context.\n",
    "        PROCESS:\n",
    "        1. Read the context thoroughly\n",
    "        2. Determine if the context contains sufficient information to answer\n",
    "        3. If sufficient: Provide a direct, accurate answer (yes/no/one word answer)\n",
    "        4. If insufficient: State \"unknown\"\n",
    "        REQUIREMENTS:\n",
    "        - Base your answer strictly on the provided context\n",
    "        - Do not use external knowledge\n",
    "        - Be concise but complete\n",
    "        - Maintain factual accuracy\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Following the instructions above, provide your answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef21cbf",
   "metadata": {},
   "source": [
    "# Advanced Evaluation using RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e75b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for RAGAs evaluation...\n",
      "==================================================\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 7 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 6 unique documents\n",
      "Generated 4 query variations\n",
      "Retrieved 4 unique documents\n",
      "Data generated successfully!\n",
      "Datasets prepared!\n",
      "\n",
      "Running RAGAs evaluation...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]Exception raised in Job[10]: IndexError(list index out of range)\n",
      "Evaluating:   3%|         | 1/30 [00:01<00:56,  1.95s/it]Exception raised in Job[13]: IndexError(list index out of range)\n",
      "Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Exception raised in Job[7]: IndexError(list index out of range)\n",
      "Exception raised in Job[4]: IndexError(list index out of range)\n",
      "Evaluating: 100%|| 30/30 [00:25<00:00,  1.16it/s]\n",
      "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]Exception raised in Job[13]: IndexError(list index out of range)\n",
      "Exception raised in Job[10]: IndexError(list index out of range)\n",
      "Exception raised in Job[4]: IndexError(list index out of range)\n",
      "Evaluating:   3%|         | 1/30 [00:03<01:47,  3.72s/it]Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Exception raised in Job[7]: IndexError(list index out of range)\n",
      "Evaluating:  23%|       | 7/30 [00:07<00:19,  1.19it/s]Exception raised in Job[19]: IndexError(list index out of range)\n",
      "Exception raised in Job[16]: IndexError(list index out of range)\n",
      "Evaluating:  40%|      | 12/30 [00:11<00:15,  1.15it/s]Exception raised in Job[22]: IndexError(list index out of range)\n",
      "Exception raised in Job[25]: IndexError(list index out of range)\n",
      "Evaluating:  50%|     | 15/30 [00:16<00:16,  1.13s/it]Exception raised in Job[28]: IndexError(list index out of range)\n",
      "Evaluating: 100%|| 30/30 [00:24<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAGAs EVALUATION RESULTS\n",
      "============================================================\n",
      "Metric               Basic RAG    Enhanced RAG Improvement \n",
      "------------------------------------------------------------\n",
      "faithfulness         0.7417       0.9000       +0.1583 (+21.3%)\n",
      "answer_relevancy     nan          nan          +nan (+0.0%)\n",
      "context_precision    0.7000       1.0000       +0.3000 (+42.9%)\n",
      "------------------------------------------------------------\n",
      "\n",
      "ANALYSIS:\n",
      " Enhanced RAG improves 2/3 metrics\n",
      " Enhanced RAG significantly outperforms Basic RAG\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple RAGAs evaluation - avoiding column issues\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Use first 10 queries\n",
    "sample_queries = queries.head(10)\n",
    "\n",
    "print(\"Generating data for RAGAs evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate Basic RAG data\n",
    "basic_answers_sample = []\n",
    "basic_contexts_sample = []\n",
    "\n",
    "for i in range(10):\n",
    "    q = sample_queries.iloc[i]['question']\n",
    "    \n",
    "    # Basic search\n",
    "    q_emb = embedding_model.encode([q], normalize_embeddings=True)\n",
    "    q_emb = np.asarray(q_emb, dtype=\"float32\")\n",
    "    \n",
    "    basic_output = client.search(\n",
    "        collection_name=\"rag_mini\",\n",
    "        data=q_emb,\n",
    "        anns_field=\"embedding\",\n",
    "        limit=1,\n",
    "        output_fields=[\"id\", \"passage\"],\n",
    "        search_params={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 16}}\n",
    "    )\n",
    "    \n",
    "    basic_context = basic_output[0][0]['entity']['passage']\n",
    "    basic_contexts_sample.append(basic_context)\n",
    "    \n",
    "    # Generate basic answer\n",
    "    basic_prompt = f\"Context: {basic_context}\\nQuestion: {q}\\nAnswer:\"\n",
    "    basic_answer = generate_answer(basic_prompt)\n",
    "    basic_answers_sample.append(basic_answer.output_text)\n",
    "\n",
    "# Generate Enhanced RAG data\n",
    "enhanced_answers_sample = []\n",
    "enhanced_contexts_sample = []\n",
    "\n",
    "for i in range(10):\n",
    "    q = sample_queries.iloc[i]['question']\n",
    "    \n",
    "    # Enhanced search\n",
    "    enhanced_docs = enhanced_search_with_reranking(q, top_k=3)\n",
    "    enhanced_contexts_sample.append(enhanced_docs)\n",
    "    \n",
    "    # Generate enhanced answer\n",
    "    enhanced_answer = generate_enhanced_answer(enhanced_docs, q)\n",
    "    enhanced_answers_sample.append(enhanced_answer)\n",
    "\n",
    "print(\"Data generated successfully!\")\n",
    "\n",
    "# Prepare datasets with ALL required columns\n",
    "basic_data = {\n",
    "    \"question\": sample_queries['question'].tolist(),\n",
    "    \"answer\": basic_answers_sample,\n",
    "    \"contexts\": [[ctx] for ctx in basic_contexts_sample],\n",
    "    \"reference\": sample_queries['answer'].tolist(),  # For context_precision\n",
    "    \"ground_truths\": sample_queries['answer'].tolist()  # For other metrics\n",
    "}\n",
    "\n",
    "enhanced_data = {\n",
    "    \"question\": sample_queries['question'].tolist(),\n",
    "    \"answer\": enhanced_answers_sample,\n",
    "    \"contexts\": [[doc['passage'] for doc in ctx_list] for ctx_list in enhanced_contexts_sample],\n",
    "    \"reference\": sample_queries['answer'].tolist(),  # For context_precision\n",
    "    \"ground_truths\": sample_queries['answer'].tolist()  # For other metrics\n",
    "}\n",
    "\n",
    "# Convert to datasets\n",
    "basic_dataset = Dataset.from_dict(basic_data)\n",
    "enhanced_dataset = Dataset.from_dict(enhanced_data)\n",
    "\n",
    "print(\"Datasets prepared!\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\nRunning RAGAs evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "basic_result = evaluate(\n",
    "    dataset=basic_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision]\n",
    ")\n",
    "\n",
    "enhanced_result = evaluate(\n",
    "    dataset=enhanced_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRAGAs EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Basic RAG':<12} {'Enhanced RAG':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'context_precision']\n",
    "\n",
    "for metric in metrics:\n",
    "    basic_score = basic_result[metric]\n",
    "    enhanced_score = enhanced_result[metric]\n",
    "    \n",
    "    # Handle lists\n",
    "    if isinstance(basic_score, list):\n",
    "        basic_score = np.mean(basic_score)\n",
    "    if isinstance(enhanced_score, list):\n",
    "        enhanced_score = np.mean(enhanced_score)\n",
    "    \n",
    "    improvement = enhanced_score - basic_score\n",
    "    improvement_pct = (improvement / basic_score) * 100 if basic_score > 0 else 0\n",
    "    \n",
    "    print(f\"{metric:<20} {basic_score:<12.4f} {enhanced_score:<12.4f} {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Analysis\n",
    "improvements = 0\n",
    "for m in metrics:\n",
    "    basic_score = basic_result[m]\n",
    "    enhanced_score = enhanced_result[m]\n",
    "    \n",
    "    if isinstance(basic_score, list):\n",
    "        basic_score = np.mean(basic_score)\n",
    "    if isinstance(enhanced_score, list):\n",
    "        enhanced_score = np.mean(enhanced_score)\n",
    "    \n",
    "    if enhanced_score > basic_score:\n",
    "        improvements += 1\n",
    "\n",
    "print(f\"\\nANALYSIS:\")\n",
    "print(f\" Enhanced RAG improves {improvements}/{len(metrics)} metrics\")\n",
    "\n",
    "if improvements >= 2:\n",
    "    print(\" Enhanced RAG significantly outperforms Basic RAG\")\n",
    "elif improvements >= 1:\n",
    "    print(\" Enhanced RAG shows moderate improvements\")\n",
    "else:\n",
    "    print(\" Enhanced RAG shows limited improvements\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
